<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LLMs as Assistive Visual Information Parsers for the Visually Impaired</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Scene Descriptions for the Visually Impaired</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mohitydv09.github.io" target="_blank">Mohit Yadav</a>,</span>
                <span class="author-block">
                  <a href="https://alexanderbesch.github.io" target="_blank">Alex Besch</a>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Abbas Booshehrain</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Sentimentals<br>CSCI 5541: Natural Language Processing Final Project</span>
                    <!-- <span class="eql-cntrb"><small><br>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/report.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Report</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/mohitydv09/nlp-final-project" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Model output in navigation mode.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="640" height="250">
        <!-- Your video here -->
        <source src="static/videos/banner_video5.mp4" type="video/mp4">
      </video>    
      <h2 class="subtitle has-text-centered">
        Model output in Question/Answering mode.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            We explore the cognitive capabilities of large language models (LLMs) to reason about a scene based solely on knowledge of relevant objects and their spatial positions. We designed a specialized module that leverages these capabilities to assist visually impaired individuals in navigating and interpreting their environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
          <p>
            We formalize our problem statement as: “Given a stream of RGB-D video, how can we process this information using an LLM to generate coherent language output that informs a visually impaired person about their surroundings?”
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Idea</h2>
        <div class="content has-text-justified">
          <p>
            Our methodology uses an object detector to first identify and localize objects in the scene. These detections are then paired with depth information and provided to the LLM for reasoning. This approach allows the LLM to incorporate temporal data, enabling better understanding of the scene. Also, as object detectors are computationally lighter than VLMs, our method is less resource intensive.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="text-align: center;">
      <div class="image-display">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" style="max-width: 60%; height: auto;" />
        <h2 class="subtitle has-text-centered">
          Information flow for navigation module of our system.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            Below are the modules that we are using in our system and how they work:
          </p>
        </div>

        <!-- Navigation Module -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Navigation</h3>
          <p>
            Images captured at regular intervals by a LiDAR camera (Intel RealSense L515) are processed through an object detector to extract object labels and pixel locations, which, combined with depth information, calculate the 3D spatial positions of objects; this spatial data is processed using a custom proximity map to translate numerical data into textual descriptions that enhance the LLM's reasoning capabilities, with the LLM queried on the last 15 seconds of data and previous five responses to generate user-friendly descriptions, utilizing YOLOv11 for object detection and GPT-4o-mini as the LLM.
          </p>
        </div>

        <!-- Visual Question Answering Module -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Visual Question Answering</h3>
          <p>
            This module helps the user explore static scenes in more depth by utilizing an off-the-shelf Visual Question Answering package (BLIP) to respond to the user&apos;s multiple queries.
          </p>
        </div>

        <!-- Scene Description Module -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Scene Description</h3>
          <p>
            This module uses an off-the-shelf pretrained BLIP model to generate a caption for the image, providing a general description of the scene for use cases where the user seeks an overview.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Evaluation and Results</h2>
        <div class="content has-text-justified">
          <p>
            To evaluate our model, we opted for a human evaluation as it appeared to be the most suitable approach, given the absence of ground truth data for our work. For comparison, we used vision-anguage models (VLMs) as baselines, as they are the closest to our intended methodology.
            
            To assess the effectiveness of different methods, a Vision-Language Model (VLM) served as the control, 
            while the experimental variable was a navigation module integrated into the system. A comparative survey 
            was conducted to gather user preferences and feedback.
          </p>
          <p>
            We conducted a comparative survey to gather user feedback. Participants reviewed two sample videos, each displaying side-by-side responses from the baseline VLM and the our method. 
            We gathered data on two aspects: 
            <ol> 
              <li> a preference comparison between our method and the VLM baseline, and </li>
              <li> the perceived usefulness of the methods on a scale of 1–5. </li>
            </ol>
            The form used to collected data can be found on the link : <a href="https://forms.gle/QK8QY26M6a9SspZx8" style="color: blue; text-decoration: underline;">Link to Google Form</a>
          </p>
          <p>
            The results provided insights into user preferences and the perceived utility of each approach. Detailed findings 
            are summarized below.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <div class="content has-text-justified">
          <h3>Preference Comparison</h3>
          <table class="table is-bordered is-striped is-fullwidth" style="text-align: center">
            <thead>
              <tr>
                <th>Preference</th>
                <th>Ours</th>
                <th>Baseline</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Sample 1</td>
                <td>21/22</td>
                <td>1/22</td>
              </tr>
              <tr>
                <td>Sample 2</td>
                <td>18/21</td>
                <td>3/21</td>
              </tr>
              <tr>
                <td>Total</td>
                <td>39/43</td>
                <td>4/43</td>
              </tr>
            </tbody>
          </table>
          <h3>Net Preference Score</h3>
          Based on user preference data we calculated the net preference score as:
          <p>
            Net Preference Score = (Preference for Ours - Preference for Baseline) / Total Responses
          </p>
          Based on the 43 responses, the net preference score for our method was: <strong>81.4%</strong>

          <h3>Rating Metrics</h3>
          <p>
            We also collected ratings on a scale of 1–5 for the perceived usefulness of each method. The results from which are summarized below:
          </p>
          <table class="table is-bordered is-striped is-fullwidth" style="text-align: center;">
            <thead>
              <tr>
                <th>Metric</th>
                <th>Ours</th>
                <th>Baseline</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Average</td>
                <td>3.65</td>
                <td>2.02</td>
              </tr>
              <tr>
                <td>Standard Deviation</td>
                <td>1.16</td>
                <td>1.00</td>
              </tr>
              <tr>
                <td>Median</td>
                <td>4</td>
                <td>2</td>
              </tr>
            </tbody>
          </table>
          <p> The score for both the methods are shown in the bar graph below.</p>
          <figure>
            <img src="static/images/result_bar.png" alt="Bar graph showing results comparison" class="image">
            <figcaption>Bar graph summarizing the comparative results of Our method and the baseline(VLM).</figcaption>
          </figure>

          <h2>Conclusion</h2>
          <p>
            In this work, we explored the use of large language models as cognitive filters and parsers of depth and visual information for the specific use case of assisting visually impaired individuals. The results demonstrate that the concept is feasible and warrants further exploration with enhanced hardware and resources. Due to time and resource constraints, we tested the method in a limited number of scenarios. However, with additional results and fine-tuning using reward-based methods, we anticipate significant improvements in the system.
          </p>  
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--Reference citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <div class="content has-text-justified">
        <p>
          1. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., & Gan, C. (2023). 3D-LLM: Injecting the 3D World into Large Language Models. arXiv. https://arxiv.org/abs/2307.12981
        </p>
        <p>
          2. Li, J., Li, D., Xiong, C., & Hoi, S. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv. https://arxiv.org/abs/2201.12086
        </p>
        <p>
          3. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. arXiv. https://arxiv.org/abs/2103.00020
        </p>
        <p>
          4. Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. arXiv. https://arxiv.org/abs/1506.02640
        </p>
      </div>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
