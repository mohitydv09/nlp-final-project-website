<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LLMs as Assistive Visual Information Parsers for the Visually Impaired</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LLMs as Assistive Visual Information Parsers for the Visually Impaired</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://mohitydv09.github.io" target="_blank">Mohit Yadav</a>,</span>
                <span class="author-block">
                  <a href="https://alexanderbesch.github.io" target="_blank">Alex Besch</a>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Abbas Booshehrain</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Sentimentals<br>CSCI 5541: Natural Language Processing Final Project</span>
                    <!-- <span class="eql-cntrb"><small><br>Indicates Equal Contribution</small></span> -->
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://drive.google.com/file/d/1TtWivraFf1cAHTTH5KVR7AnczFDtxEUw/view?usp=sharing" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Report</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/mohitydv09/nlp-final-project" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Model output in navigation mode.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop width="640" height="250">
        <!-- Your video here -->
        <source src="static/videos/banner_video5.mp4" type="video/mp4">
      </video>    
      <h2 class="subtitle has-text-centered">
        Model output in Question/Answering mode.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            We explore the cognitive capabilities of large language models (LLMs) to reason about a scene based solely on knowledge of relevant objects and their spatial positions. We designed a specialized module that leverages these capabilities to assist visually impaired individuals in navigating and interpreting their environments.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Problem Definition</h2>
        <div class="content has-text-justified">
          <p>
            We formalize our problem statement as: “Given a stream of RGB-D video, how can we process this information using an LLM to generate coherent language output that informs a visually impaired person about their surroundings?”
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed Idea</h2>
        <div class="content has-text-justified">
          <p>
            Our methodology uses an object detector to first identify and localize objects in the scene. These detections are then paired with depth information and provided to the LLM for reasoning. This approach allows the LLM to incorporate temporal data, enabling better understanding of the scene. Also, as object detectors are computationally lighter than VLMs, our method is less resource intensive.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Single Image Display -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="text-align: center;">
      <div class="image-display">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT" style="max-width: 60%; height: auto;" />
        <h2 class="subtitle has-text-centered">
          Information flow for navigation module of our system.
        </h2>
      </div>
    </div>
  </div>
</section>
<!-- End Single Image Display -->




<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Methods</h2>
        <div class="content has-text-justified">
          <p>
            Below are the modules that we are using in our system and how they work:
          </p>
        </div>

        <!-- Navigation Module -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Navigation</h3>
          <p>
            Images captured at regular intervals by a LiDAR camera (Intel RealSense L515) are processed through an object detector to extract object labels and pixel locations, which, combined with depth information, calculate the 3D spatial positions of objects; this spatial data is processed using a custom proximity map to translate numerical data into textual descriptions that enhance the LLM's reasoning capabilities, with the LLM queried on the last 15 seconds of data and previous five responses to generate user-friendly descriptions, utilizing YOLOv11 for object detection and GPT-4o-mini as the LLM.
          </p>
        </div>

        <!-- Visual Question Answering Module -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Visual Question Answering</h3>
          <p>
            This module helps the user explore static scenes in more depth by utilizing an off-the-shelf Visual Question Answering package (BLIP) to respond to the user&apos;s multiple queries.
          </p>
        </div>

        <!-- Scene Description Module -->
        <div class="content has-text-justified">
          <h3 class="title is-4">Scene Description</h3>
          <p>
            This module uses an off-the-shelf pretrained BLIP model to generate a caption for the image, providing a general description of the scene for use cases where the user seeks an overview.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results and Comparison</h2>
        <div class="content has-text-justified">
          <p>
            To assess the effectiveness of different methods, a Vision-Language Model (VLM) served as the control, 
            while the experimental variable was a navigation module integrated into the system. A comparative survey 
            was conducted to gather user preferences and feedback.
          </p>
          <p>
            Participants reviewed two videos, each displaying side-by-side responses from the VLM and the navigation module. 
            They were asked to select their preferred response as a navigation agent and to rate the usefulness of each model 
            on a scale of 1 to 5. A sample of the survey can be found <a href="https://forms.gle/QK8QY26M6a9SspZx8" target="_blank">Here.</a>
          </p>
          <p>
            The results provided insights into user preferences and the perceived utility of each approach. Detailed findings 
            are summarized below.
          </p>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary of Results</h2>
        <div class="content has-text-justified">
          <h3>Preference Comparison</h3>
          <table class="table is-bordered is-striped is-fullwidth">
            <thead>
              <tr>
                <th>Preference</th>
                <th>Method 1</th>
                <th>Method 2</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Percent Responses (1-2 Rating)</td>
                <td>17.39%</td>
                <td>71.74%</td>
              </tr>
              <tr>
                <td>Percent Responses (3 Rating)</td>
                <td>17.39%</td>
                <td>23.91%</td>
              </tr>
              <tr>
                <td>Percent Responses (4-5 Rating)</td>
                <td>65.22%</td>
                <td>4.35%</td>
              </tr>
            </tbody>
          </table>
          <h3>Rating Metrics</h3>
          <table class="table is-bordered is-striped is-fullwidth">
            <thead>
              <tr>
                <th>Metric</th>
                <th>Method 1</th>
                <th>Method 2</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>Average</td>
                <td>3.65</td>
                <td>2.02</td>
              </tr>
              <tr>
                <td>Standard Deviation</td>
                <td>1.16</td>
                <td>1.00</td>
              </tr>
              <tr>
                <td>Minimum</td>
                <td>1</td>
                <td>1</td>
              </tr>
              <tr>
                <td>Maximum</td>
                <td>5</td>
                <td>5</td>
              </tr>
              <tr>
                <td>Median</td>
                <td>4</td>
                <td>2</td>
              </tr>
            </tbody>
          </table>
          <h3>Net Preference Score</h3>
          <p>
            Method 1: <strong>+81.4%</strong><br>
          </p>
          <p>
            Based on the results, <strong>Method 1</strong> was stronly preferred, with higher average ratings and a net positive preference score.
          </p>
          <h3>Visual Summary</h3>
          <figure>
            <img src="static/images/result_bar.png" alt="Bar graph showing results comparison" class="image">
            <figcaption>Bar graph summarizing the comparative results of Method 1 and Method 2.</figcaption>
          </figure>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Main Findings</h2>
        <div class="content has-text-justified">
          <p>
            In our qualitative tests, we found that the LLM demonstrates strong coherence in reasoning about the relevant objects, based on their location and object labels. Additionally, we observed that textual information, rather than numerical data, aids the model in better understanding the scene.
          </p>
          <h3>Potential for Publication</h3>
          <p>
            This project introduces a novel method combining depth data with language models to assist visually impaired individuals. 
            While promising, challenges like limited replicability, proprietary models, and a small evaluation sample (22 data points) 
            need to be addressed. Expanding testing and exploring open-source alternatives could strengthen its publication potential.
          </p>
          <h3>Potential Impact</h3>
          <p>
            The system leverages deep learning and Vision-Language Models to enhance navigation for visually impaired individuals by 
            integrating RGB and depth data for accurate distance perception. This technology could greatly benefit users, assistive 
            tech companies, and researchers, advancing independence and safety.
          </p>
          <h3>Replicability</h3>
          <p>
            Challenges in replicating this work include reliance on proprietary models, custom datasets, and specialized hardware (Intel 
            RealSense Lidar Camera L515). The complex pipeline, involving parallel object detection, localization, and language inference, 
            also limits replicability.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Limitations and Discussion</h2>
        <div class="content has-text-justified">
          <p><strong>Key limitations include:</strong></p>
          <ul>
            <li><strong>Object Detection Challenges:</strong> The pre-trained YOLO model used is limited to 80 classes and occasionally produces inaccurate labels, an inherent limitation of current computer vision models.</li>
            <li><strong>Hardware Constraints:</strong> The Intel LiDAR camera is suitable only for indoor use and objects within 9 meters, restricting system applicability.</li>
            <li><strong>Inference Timing:</strong> Fixed time windows for LLM responses limited flexibility. Dynamic, context-aware inferences could improve the system's responsiveness.</li>
            <li><strong>Scalability:</strong> While we developed three functional modules, expanding these and integrating them with an intent-recognizing LLM could significantly enhance the system.</li>
          </ul>
          <p>
            Future work will focus on addressing these limitations and broadening the system's capabilities.
          </p>
          <p>
            Our method relies on object detection and depth information from the camera, which may not always be accurate. While the approach shows promising results, there is significant potential for improvement in the system design to better leverage the capabilities of the LLM. For instance, incorporating IMU data could help the system better reason about the user's movements.
          </p>
          <p>
            Our system demonstrated clear improvements over using a Vision-Language Model (VLM) for navigation tasks, showcasing potential for enhanced performance. However, its overall utility remains inconclusive due to a limited dataset and the absence of evaluations with visually impaired users.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->



<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--Reference citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">References</h2>
      <div class="content has-text-justified">
        <p>
          1. Hong, Y., Zhen, H., Chen, P., Zheng, S., Du, Y., Chen, Z., & Gan, C. (2023). 3D-LLM: Injecting the 3D World into Large Language Models. arXiv. https://arxiv.org/abs/2307.12981
        </p>
        <p>
          2. Li, J., Li, D., Xiong, C., & Hoi, S. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. arXiv. https://arxiv.org/abs/2201.12086
        </p>
        <p>
          3. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., & Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. arXiv. https://arxiv.org/abs/2103.00020
        </p>
        <p>
          4. Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You Only Look Once: Unified, Real-Time Object Detection. arXiv. https://arxiv.org/abs/1506.02640
        </p>
      </div>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
